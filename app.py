import streamlit as st
from transformers import RobertaTokenizer, RobertaForSequenceClassification
import torch
import torch.nn.functional as F
from peft import PeftModel  # <--- NEW IMPORT

# --- 1. CONFIGURATION ---
MODEL_PATH = "./model"
st.set_page_config(page_title="AI Text Detector", page_icon="ðŸ¤–")

# --- 2. LOAD MODEL (Cached so it doesn't reload every click) ---
@st.cache_resource
def load_model():
    try:
        # 1. Load the Tokenizer from your local folder (since you have vocab.json there)
        tokenizer = RobertaTokenizer.from_pretrained(MODEL_PATH)
        
        # 2. Load the BASE model from the internet (Facebook's original model)
        #    We need this because your folder only contains the "updates", not the whole brain.
        base_model = RobertaForSequenceClassification.from_pretrained(
            "FacebookAI/roberta-base", 
            num_labels=2
        )
        
        # 3. Load YOUR trained adapters on top of the base model
        model = PeftModel.from_pretrained(base_model, MODEL_PATH)
        
        # Move to GPU if available, otherwise CPU
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model.to(device)
        
        return tokenizer, model
        
    except Exception as e:
        st.error(f"Error loading model: {e}")
        return None, None

tokenizer, model = load_model()

# --- 3. PREDICTION FUNCTION ---
def predict(text):
    # Prepare input
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    
    # Get prediction
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
    
    # Calculate probabilities
    probs = F.softmax(logits, dim=-1)
    
    # Assuming Label 1 is AI, Label 0 is Human (Based on our training)
    ai_prob = probs[0][1].item()
    human_prob = probs[0][0].item()
    
    predicted_class = torch.argmax(probs).item()
    confidence = ai_prob if predicted_class == 1 else human_prob
    
    return predicted_class, confidence, {"AI": ai_prob, "Human": human_prob}

# --- 4. THE UI ---
st.title("ðŸ¤– AI vs. Human Text Detector")
st.write("Paste text below to check if it was likely generated by an AI model like ChatGPT.")

text_input = st.text_area("Enter text here:", height=200)

if st.button("Analyze Text"):
    if not text_input.strip():
        st.warning("Please enter some text first.")
    elif tokenizer is None:
        st.error("Model not found. Please check your folder path.")
    else:
        with st.spinner("Analyzing..."):
            pred_class, conf, probs = predict(text_input)
            
            # Display Results
            st.divider()
            
            # Logic: 0 = Human, 1 = AI
            if pred_class == 1:
                st.error(f"ðŸš¨ **AI-Generated Content Detected**")
                bar_color = "red"
            else:
                st.success(f"âœ… **Likely Human-Written**")
                bar_color = "green"
            
            st.metric("Confidence Score", f"{conf:.2%}")
            
            # Progress bar visualization
            st.write("Probability Breakdown:")
            col1, col2 = st.columns(2)
            with col1:
                st.info(f"Human: {probs['Human']:.2%}")
                st.progress(probs['Human'], text="Human Probability")
            with col2:
                st.warning(f"AI: {probs['AI']:.2%}")
                st.progress(probs['AI'], text="AI Probability")

# Add a sidebar for info
with st.sidebar:
    st.header("About")
    st.write("This model uses a fine-tuned RoBERTa transformer to classify text patterns.")
    st.write("**Labels:**")
    st.write("- 0: Human")
    st.write("- 1: AI")